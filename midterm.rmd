---
output: 
  html_document: 
    css: background.css
    theme: cosmo
---
---
title: "Midterm"
author: "Bob Bartolini"
date: "11/2/2020"
output: html_document
  

---


https://github.com/rjmaitri/Midterm.git

```{r setup, include=FALSE}
#scrolling code output
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

```{r}
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(plotly)
library(profileModel)
library(brms)
library(bayesplot)
library(reactable)
library(tidybayes)
library(bayesplot)
library(rstanarm)
library(modelr)
options(mc.cores = parallel::detectCores())
```

### 1) Sampling your system (10 points)
#### Each of you has a study system your work in and a question of interest.

<span style="color: green;">High-throughput sequencing technology provides data on transcript abundance and diversity. This technology allows us to interrogate the transcriptomic effects of chemotherapeutic drug treatments on breast cancer cell (BCC) lines. We can interrogate the transcriptional effects of chemotherapeutics by culturing a BCC line with different combinations of chemotherapuetic agents, thus, interrogate the transcriptional changes that occur under chemothrapy.</span>

<span style="color: green;">The negative binomial distribution is suited for this data because it is count data with a variance that shifts away from the mean with greater gene expression. The variance behaves this way because experiments typically have 3-5 samples with a few highly expressed genes and many, lowly expressed genes. The negative binomial distribution is suited for this data, as it accomodates the variance shifting away from the mean with higher levels of gene expression. Theoretically, a poisson distribution would work with a larger sample size, however, this is financially burdensome.</span>

<span style="color: green;">Random sampling from breast cancer tissue throughout the population would be ideal, however 2/3rds of all in vitro breast cancer research uses three BCC lines, with the the oldest line has been cultured since 1970 and retains many features of the tumor that it originated from. Biological triplicates and experimental replicates enhance the validity of estimation. 
Improper experimental design and execution create batch effects. Asynchrounously running the experiments, different reagents, inconsistent cell culture techniques can create batch and counfounding effects on the results of the experiment. These need to be minimized in order to provide credibility to the recorded effects of the chemotherapeutic agents.</span> 

<span style="color: green;">Further, the genomic instability of cancer creates the possibility for confounding effects, as the differential gene expression seen across treatments may be related to differences within the sample itself.</span>

2) Data Reshaping and Visuzliation
Johns Hopkins has been maintaining one of the best Covid-19 timseries data sets out there. The data on the US can be found here with information about what is in the data at https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data



2a) Access (5 points)
Download and read in the data. Can you do this without downloading, but read directly from the archive (+1).

```{r}
#read raw github from internet

Covid_JHU <- read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")

#look at the file
head(Covid_JHU)


```

2b) It’s big and wide! (10 Points)
The data is, well, huge. It’s also wide, with dates as columns. Write a function that, given a state, will output a time series (long data where every row is a day) of cummulative cases in that state as well as new daily cases.

```{r}

#write a function/figure out way to update by day

state_function <- function(x) {

#select states and dates
clean_covid <- Covid_JHU %>%
  select(state = 7, 12:305)



#insert pivot long here

#pivot long
Covid_long <- pivot_longer(clean_covid,
                           cols = !state,
                           names_to = "Date",
                           values_to = "Cases")

#lubridates

Covid_dates <- Covid_long %>% 
  mutate(Date = mdy(Date))

#group by date, state and summarize cases

tidying_up <- Covid_dates %>%
      
      group_by(Date, state) %>%

      summarise(Cases = sum(Cases))

#filter by function input
tidy_covid <- tidying_up %>%
  rowwise() %>%
  filter(state == x) 

return(tidy_covid)


}

Mass_data <- state_function("Massachusetts")

reactable(Mass_data)
```

2c) Let’s get visual! (10 Points)
Great! Make a compelling plot of the timeseries for Massachusetts! Points for style, class, ease of understanding major trends, etc. Note, 10/10 only for the most killer figures. Don’t phone it in! Also, note what the data from JHU is. Do you want the cummulatives, or daily, or what?

```{r}


p <- Mass_data %>%
  ggplot( aes(x=Date, y=Cases)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("Covid 19 Infections)") +
    theme_dark()

# Turn it interactive with ggplotly
p <- ggplotly(p)
p



```



2d) At our fingertips (10 Points)
Cool. Now, write a function that will take what you did above, and create a plot for any state - so, I enter Alaska and I get the plot for Alaska! +2 if it can do daily or cumulative cases - or cases per 100,000 if you did that above. +3 EC if you highlight points of interest - but dynamically using the data. Note, you might need to do some funky stuff to make things fit well in the plot for this one. Or, meh.

```{r}
#modularize state data and plot
state_plot <- function(x){
  
  #state function
  data <- state_function(x)
  
#create a ggplot object with state data 
  p <- data %>%
  ggplot( aes(x=Date, y=Cases)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("Covid 19 Infections)") +
    theme_dark()

# Turn it interactive with ggplotly
out <- ggplotly(p)

  
  return(out)
}

state_plot("Alabama")
```


3) Let’s get philosophical. (10 points)
We have discussed multiple inferential frameworks this semester. Frequentist NHST, Likelihood and model comparison, Baysian probabilistic thinking, Assessment of Predictive Ability (which spans frameworks!), and more. We’ve talked about Popper and Lakatos. Put these pieces of the puzzle together and look deep within yourself.



What do you feel is the inferential framework that you adopt as a scientist? Why? Include in your answer why you prefer the inferential tools (e.g. confidence intervals, test statistics, out-of-sample prediction, posterior probabilities, etc.) of your chosen worldview and why you do not like the ones of the other one. This includes defining just what those different tools mean, as well as relating them to the things you study. extra credit for citing and discussing outside sources - one point per source/point

<span style="color: green;">I am inclined towards inferential thought processes that resemble Bayesian probabilistic thinking, as I tend to make causal inferences by observing patterns. Further, this inferential thought process is influenced by my prior knowledge which is analguous to a conditional probability. Although, my mind is often misled by conditional probabilities if the math is not explicitly shown. For example, I don't perform the mental math necessary to determine a posterior probability of having a disease given a positive test result. The statistics required to formulate this posterior can be easily misinterpreted if taken in piecemeal. A low false-positive rate and a relatively high test result specificity give the impression that a P(have the disease|positive test result) is highly likely, however, the sample size of the false positive rate needs to be taken into account. After factoring in sample size, the chance of not having the disease can overwhelm the posterior and contrast my initial appraisal.</span>

<span style="color: green;">As an undergraduate scientist, Frequentist Null Hypthoesis Significance Testing has been the primary inferential framework used within my studies. For instance, hypotheses in Genetics (BIOL254) are tested by calculating probabilities for allele inheritance by applying expected and observed frequencies to the chi-square distribution using the appropriate degrees of freedom. The probability value generated from the chi-square distribution is then analyzed against a threshold value for significance. P-values below the threshold are considered statisically significant and fails to accept falsification. 

                #######PICK UP AT COUNT DATA AND LIKELHOOD ###################



4) Bayes Theorem (10 points)
I’ve referenced the following figure a few times. I’d like you to demonstrate your understanding of Bayes Theorem by hand (e.g. calculate it out and show your work - you can do this all in R, I’m not a monster) showing what is the probability of the sun exploding is given that the device said yes. Assume that your prior probability that the sun explodes is p(Sun Explodes) = 0.0001 (I’ll leave it to you to get p(Sun Doesn’t Explode). The rest of the information you need - and some you don’t - is in the cartoon - p(Yes | Explodes), p(Yes | Doesn’t Explode), p(No | Explodes), p(No | Doesn’t Explode).

```{r}

#P(Explodes|Yes)= P(Yes|Explodes)p(sun explodes)/p(sun does not explode) 



```

4a Extra Credit (10 Points)
Why is this a bad parody of frequentist statistics?

<span style="color: green;">The p-value of 1/36 is specific to obtaining two dice rolls of 6 and is independent of the probablity of the sun exploding. Therefore, this is a bad parody because the null hypothesis that the sun did not explode is rejected using the independent probability of obtaining two sixes. While it is unlikely that the machine lied, it is far more unlikely that the sun went nova.</span>



5) Quailing at the Prospect of Linear Models
I’d like us to walk through the three different ‘engines’ that we have learned about to fit linear models. To motivate this, we’ll look at Burness et al.’s 2012 study "Post-hatch heat warms adult beaks: irreversible physiological plasticity in Japanese quail http://rspb.royalsocietypublishing.org/content/280/1767/20131436.short the data for which they have made available at Data Dryad at http://datadryad.org/resource/doi:10.5061/dryad.gs661. We’ll be looking at the morphology data.

```{r}
#load qual morphology data
quail_data <- read.csv(na.omit("data/Morphology data.csv"))

#structure of data frame
str(quail_data)

```

```{r}
skimr::skim(quail_data)

```

5a) Three fits (10 points)
To begin with, I’d like you to fit the relationship that describes how Tarsus (leg) length predicts upper beak (Culmen) length. Fit this relationship using least squares, likelihood, and Bayesian techniques. For each fit, demonstrate that the necessary assumptions have been met. Note, functions used to fit with likelihood and Bayes may or may not behave well when fed NAs. So look out for those errors.

```{r}
#generate a plot of the quail data
quail_plot <- ggplot(data = quail_data,
       mapping = aes(x = Tarsus..mm., y = Culmen..mm.)) +
  geom_point(alpha = 0.5)+
  theme_classic()

#plot with line
quail_plot + 
  stat_smooth(method=lm, formula=y~x)
```

<span style="color: green;">The variance of residuals appear to violate homoscedasticity, evident by the increase in variation as the mean increases. This indicates a glm may be better suited, as it reweights the larger variances.</span>

```{r}
#fit the relationship of tarsus length predicting upper beak using least squares

LSquail <- lm(Culmen..mm. ~ Tarsus..mm., data = quail_data)

#simulate the data
LSquail_sims <- simulate(LSquail, nsim = 20) %>%
  pivot_longer(
    cols = everything(),
    names_to = "sim",
    values_to = "Culmen..mm."
  )
#plot distribution of simulated data over observed data
ggplot() +
  geom_density(data = LSquail_sims,
               mapping = aes(x = Culmen..mm., group = sim), 
               size = 0.2)  +
  geom_density(data = quail_data,
               mapping = aes(x = Culmen..mm.),
               size = 2, color = "blue")

```

<span style="color: green;">The simulated data does not recapitulate our observed values.</span>

```{r}

#check the relationship of residual vs. fitted values
plot(LSquail, which =1)

```

<span style="color: green;">The residuals vs. fitted plot appears to be clustered in two regions, however, this may be due to small sample size as residuals have a cloud formation. However, this may be suggestive of non-linearity and the need to restructure the model.</span>

```{r}
#check normality of residuals
residuals(LSquail) %>% hist()

```
<span style="color: green;">Residuals are normally distributed.</span>

```{r}

plot(LSquail, which = 2)



```

<span style="color: green;">This qqplot suggests we investigate the model.</span>

Likelihood Regression


```{r}

#GLM for quail data
ggplot(quail_data, 
       mapping = aes(x = Tarsus..mm., y = Culmen..mm.)) +
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = gaussian(link="identity")))

```

```{r}

#fit a model using the iteratively reweighted least squares algorithm in GLM
quail_mle <- glm(Culmen..mm. ~ Tarsus..mm.,
                 #identity means 1:1 translation between linear predictors and shape of curve
                family = gaussian(link = "identity"),
                data = quail_data)



```




```{r}
#extract predicted and residuals for plot
fitted <- predict(quail_mle)
res <- residuals(quail_mle)

qplot(fitted, res)
```


```{r}
qqnorm(res)
```

<span style="color: green;">The linearity of the GLM qqplot is better behaved than the LM plot</span>

```{r}
hist(res)

```

<span style="color: green;">Residuals have a normal distribution.</span>



Bayes Regression

```{r}
options(mc.cores = parallel::detectCores())
set.seed(600)
color_scheme_set("yellow")
quail_brm <- brm(Culmen..mm. ~ Tarsus..mm.,
                  data = quail_data,
                  family = gaussian(link = "identity"))

plot(quail_brm)

```

<span style="color: green;">The parameters are behaved and the chains show convergence</span>

```{r}

#does our data match the chains for y-distributions?
color_scheme_set("green")
pp_check(quail_brm, "dens_overlay")+
  theme_light()

```


<span style="color: green;">The data contains trends that are not well-reflected in the predicted values.</span>




```{r}
#fitted vs. residual, do we meet linearity assumption?
quail_fit <- fitted(quail_brm) %>% as.data.frame()
quail_res <- residuals(quail_brm) %>% as.data.frame()

plot(quail_fit$Estimate, quail_res$Estimate)

```

<span style="color: green;">The residuals vs fitted appears to cluster at two points, suggesting that this model violates the assumption of linearity.</span>

```{r}

qqnorm(quail_res$Estimate)
qqline(quail_res$Estimate)

```




5b) Three interpretations (10 points)
OK, now that we have fits, take a look! Do the coefficients and their associated measures of error in their estimation match? How would we interpret the results from these different analyses differently? Or would we? Note, confint works on lm objects as well.

```{r}
#LM coef

LSquail <- lm(Culmen..mm. ~ Tarsus..mm., data = quail_data)

coef(LSquail)

####

```

5c) Everyday I’m Profilin’ (10 points)
For your likelihood fit, are your profiles well behaved? For just the slope, use grid sampling to create a profile. You’ll need to write functions for this, sampling the whole grid of slope and intercept, and then take out the relevant slices as we have done before. Use the results from the fit above to provide the reasonable bounds of what you should be profiling over (3SE should do). Is it well behaved? Plot the profile and give the 80% and 95% CI (remember how we use the chisq here!). Verify your results with profileModel.



```{r}
#####
#For just the slope, use grid sampling to create a profile. 
#You’ll need to write functions for this, sampling the whole grid of slope and intercept, 
#and then take out the relevant slices as we have done before. 

#Use the results from the fit above to provide the reasonable bounds of what you should be profiling over 
#(3SE should do). Is it well behaved? Plot the profile and give the 80% and 95% CI (remember how we use the chisq here!).
#Verify your results with profileModel.


norm_likelihood <- function(obs, mean_est, sd_est){
  
  #data generating process
  est <- mean_est
  
  #log likelihood
  sum(dnorm(obs, mean = est, sd = sd_est, log = TRUE))
  
}
quail_dist <- crossing(m = seq(30, 40, by = 0.1),
                       s=seq(5, 15, by = 0.1)) %>%
  rowwise() %>%
  mutate(log_lik = norm_likelihood(obs = na.omit(quail_data$Tarsus..mm.), mean_est = m, sd_est = s)) %>% 
  ungroup()

```

```{r}
quail_dist %>%
  filter(log_lik == max(log_lik))
```

```{r}
#profile for the mean
quail_sd_profile <- quail_dist %>%
  #group by sd
  group_by(s) %>%
  #filter out mle
  filter(log_lik == max(log_lik)) %>%
  ungroup()

qplot(s, log_lik, data=quail_sd_profile, geom = "line")

```



```{r}
ninetyfive_CI <- quail_sd_profile %>%
  filter(log_lik > max(log_lik) - qchisq(0.95, df = 1)/2) %>%
    filter(row_number()==1 | row_number()==n())

ninetyfive_CI
```

```{r}
eighty_CI <- quail_sd_profile %>%
  filter(log_lik > max(log_lik) - qchisq(0.80, df = 1)/2) %>%
    filter(row_number()==1 | row_number()==n())

eighty_CI

```

```{r}
quail_m_profile <- quail_dist %>%
  group_by(s) %>%
  filter(log_lik == max(log_lik)) %>%
  ungroup()

qplot(s, log_lik, data=quail_m_profile, geom = "line")+
  geom_vline(xintercept=6.6, color = "red")+
  geom_vline(xintercept=6.9, color = "red")+
  geom_vline(xintercept=6.5, color = "blue")+
  geom_vline(xintercept=7, color = "blue")+
  theme_minimal()

```

```{r}
#verify 95% CI results
prof <- profileModel(quail_mle,
                     objective = "ordinaryDeviance",
                     quantile = qchisq(0.95,1))
plot(prof)


```




```{r}
#verify 80% CI results
prof80 <- profileModel(quail_mle,
                     objective = "ordinaryDeviance",
                     quantile = qchisq(0.80,1))
plot(prof80)

```


5d) The Power of the Prior (10 points)
This data set is pretty big. After excluding NAs in the variables we’re interested in, it’s over 766 lines of data! Now, a lot of data can overwhelm a strong prior. But only to a point. Show first that there is enough data here that a prior for the slope with an estimate of 0.7 and a sd of 0.01 is overwhelmed and produces similar results to the default prior. How different are the results from the original?



```{r}
adj_brm_prior <- brm(Culmen..mm. ~ Tarsus..mm.,
                         data = quail_data,
                         family=gaussian(),
                   prior = (prior(normal(0.7, 0.01), class = b)))
adj_brm_prior
```

```{r}
#Prepare coefficients and credible intervals from the BRMs for graphing

#adjusted prior coefficient and credible interval
adjPriorCoef <- fixef(adj_brm_prior)
adjCoef_wide <- spread_draws(adj_brm_prior,
                             b_Intercept,
                             b_Tarsus..mm.)

#default prior coefficient and credible interval
quail_coefs <- fixef(quail_brm)
quail_coefs_wide <- spread_draws(quail_brm,
                                b_Intercept,
                                b_Tarsus..mm.)
```

```{r}

#tarsus v. culmen plot
quail_plot +  
#credible interval of default prior
  geom_abline(data = quail_coefs_wide,
              aes(slope = b_Tarsus..mm.,
                  intercept = b_Intercept),
              color = "blue",
              alpha = 0.06) +
  #median fit
  geom_abline(slope = quail_coefs[2,1],
              intercept = quail_coefs[1,1],
              color = "red",
              size = 1)+
# credible interval of adjusted prior   
  geom_abline(data = adjCoef_wide,
              aes(slope = b_Tarsus..mm.,
                  intercept = b_Intercept),
              color = "orange",
              alpha = 0.08) +
  #median fit of adjusted prior model
  geom_abline(slope = adjPriorCoef[2,1],
              intercept = adjPriorCoef[1,1],
              color = "green",
              size = 1)




```


Second, randomly sample 10, 100, 300, and 500 data points. At which level is our prior overwhelmed (e.g., the prior slope becomes highly unlikely)? Communicate that visually in the best way you feel gets the point across, and explain your reasoning.


```{r}


#draw random samples from quail data


Functionally <- function(x) {

samples <- quail_data[sample(nrow(quail_data), x), ] %>%
  na.omit()

#input into BRMS

Brm_object <- brm(Culmen..mm. ~ Tarsus..mm.,
                  data = samples,
                  family = gaussian(link = "identity"),
                  chains = 1,
                  iter = 500)

#get_prior

out <- plot(Brm_object)


return(Brm_object)

}

tensamps <- Functionally(10)
```

```{r}

hundredsamps <- Functionally(100)

plot(hundredsamps, par = "b_Tarsus..mm.")

prior(hundredsamps, par = "b_Tarsus..mm.")
```

```{r}

threehunsamps <- Functionally(300)


```

```{r}

fivehunsamps <- Functionally(500)

```

```{r}

#ten sample coefs and draws
tensampCoef <- fixef(tensamps)
tensampcoefs_wide <- spread_draws(tensamps,
                                b_Intercept,
                                b_Tarsus..mm.)

#get posterior


#100 sample coefs and draws
hundredsampCoef <- fixef(hundredsamps)
hundredcoefs_wide <- spread_draws(hundredsamps,
                                b_Intercept,
                                b_Tarsus..mm.)

#500 sample coefs and draws
fivesampCoef <- fixef(fivehunsamps)
fivesampcoefs_wide <- spread_draws(fivehunsamps,
                                b_Intercept,
                                b_Tarsus..mm.)


```


```{r}

quail_plot +  
#simulated draws from ten samps
  geom_abline(data = tensampcoefs_wide,
              aes(slope = b_Tarsus..mm.,
                  intercept = b_Intercept),
              color = "orange",
              alpha = 0.26) +
  #median fit of ten samples
  geom_abline(slope = tensampCoef[2,1],
              intercept = tensampCoef[1,1],
              color = "yellow",
              size = 1)+
  #simulated draws of 100 samples
  geom_abline(data = hundredcoefs_wide,
              aes(slope = b_Tarsus..mm.,
                  intercept = b_Intercept),
              color = "purple",
              alpha = 0.16) +
  
  #median fit of 100 samples
  geom_abline(slope = hundredsampCoef[2,1],
              intercept = hundredsampCoef[1,1],
              color = "white",
              size = 1)+
  
  
# credible interval of 500 samps  
  geom_abline(data = fivesampcoefs_wide,
              aes(slope = b_Tarsus..mm.,
                  intercept = b_Intercept),
              color = "orange",
              alpha = 0.08) +
  #median fit of 500 samps
  geom_abline(slope = fivesampCoef[2,1],
              intercept = fivesampCoef[1,1],
              color = "green",
              size = 1)


```

```{r}

model <- stan_glm(Culmen..mm. ~ Tarsus..mm., data = quail_data,
                  prior = normal(0, 3, autoscale = FALSE))

get_prior(tensamps)
```


+4 for a function that means you don’t have to copy and paste the model over and over. + 4 more if you use map() in combination with a tibble to make this as code-efficient as possible. This will also make visualization easier.

6) Cross-Validation and Priors (15 points)
There is some interesting curvature in the culmen-tarsus relationship. Is the relationship really linear? Squared? Cubic? Exponential? Use one of the cross-validation techniques we explored to show which model is more predictive. Justify your choice of technique. Do you get a clear answer? What does it say?





#####citations
https://link.springer.com/article/10.1023/B:BREA.0000014042.54925.cc (2/3 ~ 3 BCC lines)